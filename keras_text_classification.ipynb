{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tensorflow 2.0 keras text classification '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "df1 = df1.drop(['id'], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(['Unnamed: 0', 'type','file'], axis=1)\n",
    "df2.columns = ['review', 'sentiment']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2.sentiment != 'unsup']\n",
    "df2['sentiment'] = df2['sentiment'].map({'pos' : 1, 'neg': 0})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2]).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重点：去除停用词、构建特征集\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer # 词干\n",
    "from nltk.corpus import stopwords # 停用词\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # re.sub(pattern, replace, text, ...) \n",
    "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE) # 将单字符或者特殊字符替换成 空\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(' ')]\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['Processed_Reviews'] = df['review'].apply(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Reviews'].apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(df['ProcessLookupError'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])\n",
    "\n",
    "# padding\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "build_model():\n",
    "    embed_size = 128\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_size))\n",
    "    # 双向lstm效果好， return_sequences = True表示每一个隐含层的ht都返回\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # kernel_regularizer可以选择l2或者l1\n",
    "    model.add(Dense(20, kernel_regularizer=regularizers.l2(0.0001), activation=\"relu\"))\n",
    "    # dropout rate\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.001,\n",
    "        decay_steps=STEPS_PER_EPOCH*1000,\n",
    "        decay_rate=1,\n",
    "        staircase=False)\n",
    "\n",
    "    def get_optimizer():\n",
    "        return tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=get_optimizer(), \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 3\n",
    "\n",
    "# patience 值用来检查改进 epochs 的数量\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_t, y, batch_size = batch_size, epochs = epochs, validation_split = 0.2, verbose=2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"../input/word2vec-nlp-tutorial/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_test.head()\n",
    "df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\n",
    "df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = df_test[\"sentiment\"]\n",
    "list_sentences_test = df_test[\"review\"]\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
